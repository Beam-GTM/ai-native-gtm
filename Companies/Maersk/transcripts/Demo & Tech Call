Quentin Silvestro: Hello. Hello.
Craig Simpson: Oh, Quentin. Good afternoon. How are you doing?
Quentin Silvestro: Great. It's a beautiful Thursday afternoon in Germany. Well, it's raining right now. Ben, are you back in Portugal? It's the weather.
Craig Simpson: Nice.
Quentin Silvestro: I've brought Bennett with me, in case you were wondering. Ben is our cpo. He lives in Portugal. The weather is great there.
Benedikt Sanftl: That's what people think. Usually it's stormy and rainy and windy and cold and not cold.
Craig Simpson: I was just on a call with one of my colleagues who's in Portugal and she said it's thunderstorms. I don't know where she's residing exactly, but it's the same case in Miami as well. It's raining, thunderstorms, lightning, etc. But it's just started to clear up in the last few minutes and we.
Benedikt Sanftl: Seem to get more and more of this climate. It used to be not like this, but since the last years we tend to get more close to Portugal. Storms, they used to be more north, like going through France and Ireland and England, but now they seem to come more south. So, yeah, wherever this comes from.
Craig Simpson: It comes from us. No doubt. Comes from the U.S.
Quentin Silvestro: That'S probably true. I hope the weather clears up where you are correct, because I'm coming your way in two weeks and I would love to enjoy nice weather and sun.
Craig Simpson: I fully expect it to be exactly like this in two weeks.
Quentin Silvestro: Well, okay, I see we are probably completed now, right? Is everybody there from the merch side?
Craig Simpson: Yeah, we've got one more. But he just put a message, I think maybe directly to me who's going to join in a few minutes so we can kick it off for the time being.
Quentin Silvestro: Cool. Okay. As Craig, you and I have only spoken so far. Should we do a quick round of introductions and then as we only have 30 minutes, let's jump right into the platform and then open the room for Q and A. I think that's good as I'm already speaking. Hey guys, my name is Quentin. I'm one of the commercial people here at beam, leading the commercial division and building, I would say our go to market since one and a half years now, Ben. It's been a ride. I always say my role in this company is to build these agentic journeys, which basically means looking at companies like you and think, okay, this is now an organization that's fully run by human beings. Where can we start building or plugging in these agentic solutions to future? Map out the picture of building this AI native company, which we always like to say, but I'll keep it short. I'VE brought Bennett with us today. Bennett, want to quickly take it?
Benedikt Sanftl: Yes. I'm the CPO and I'm building in with the team. What yeah. Later becomes the native journey. And yeah, always love to also have direct contact with clients to learn from you guys. I also have like the team of. I also run the team of solution engineers. So keep product solutions clients close together so we can go on this journey together. Prior to bim, I was part of the, let's say middle phase of the cruise where we scaled the cruise to the Robotaxi company in San Francisco. So I was in this hyper growth phase there. Left them two and a half years ago to build Beam. Yeah, unfortunately now it kind of is not there anymore. But that was very exciting to be part of the San Francisco VC startup case. So it's. Yeah. So this is my background. I started doing with AI and all of this agentic stuff in a big project with Amazon and BMW when we like one of my co founders and me, we put the first Alexa into a BMW. So we kind of tore apart one of the early speakers, took the software, cross compiled it, put it in the car, showed it to management and then seven years later automated innovation and they put it in the seven series, so which is for sale since last summer. So I did the first three years and then four years after I left, kind of five years afterwards it became a serious product. So yeah, but that's when I first played with GPT2 back in beta, API and all that stuff. And I honestly have to say I missed it back then. I couldn't imagine how crazy it would go from GPT2 now to everything we have now of anthropic, OpenAI, Gemini, etc. So yeah, that is insane. But this is why we're speaking here today.
Craig Simpson: Yeah, I can kick it off from the mer side. So my name is Craig Simpson. I'm part of our team called the Global LNS Innovation Team, which stands for logistics and Services. Just to keep it short and sweet, my responsibilities is to evaluate external solution partners to bring in house to solve problems that perhaps we cannot or don't necessarily perhaps have the time to market to solve. So holistically speaking, I'll be the project manager for argument's sake. But yeah. Nicole, you want to take it?
Speaker 4: Yeah, I'll keep it simple. So my name is Nicolcia, I reside within Nordics in Europe and my role at Maersk is the global head of Strategy, Custom strategy.
Speaker 5: And I'm Mark, I am head of innovation globally for Maersk prior to that regional head of customs for both Canada, US and Mexico. 40 plus year career on the custom side through multiple organizations. So all things customs, that's kind of where I fit.
Benedikt Sanftl: Anil.
Craig Simpson: Hey.
Speaker 5: Hey, hey. Sorry for apologies for joining late. Myself is Anil. I was a lead data scientist over here. I mean so we predominantly work on agenda case stuff. We build some a kind of a workflow automation. So mainly for in the areas of our CX agents actually yeah, that's it about for me.
Quentin Silvestro: Cool. Craig, do you already have a list of questions prepared or should I quickly introduce into the platform and what we've did last time on a business level and then Ben and tech people can exchange.
Craig Simpson: Yeah, I think that's a good plan just to get us kicked off so we can ask questions as we go along. Cool.
Quentin Silvestro: Bennett. I will set the scene with my business level demo of the platform and then you can chip in whenever you think it's needed. Cool. Then you should be able to see yourself in a second. And now you should see my platform. Is that the case? I cannot see you anymore. So please give me a sign. You see vmai. That's great. So this is the platform Craig, I've already showed you this so it's going to be a bit repetitive for you but I'm going to high level walk through the different modules. How do we set up agents and how do we think about agents and then we can zoom into the technological parts of the product whenever it's needed. So what you're seeing right here is our home screen. You get greeted with a natural language interface in which you can start creating agents via text. This will take you to a guided flow to create one of these agents that we have here on the left hand side. For the sake of time. I will skip this for a second in the in the deck and video that I shared with you guys. I don't know if everybody had the chance to look at it. There's also a video of me recorded where I do all of this so you can rewatch that. Now on the left hand side you do see two agents. For the last call with Craig I had a custom customer service agent. Not a custom service agent had nothing to do with this. Just as a broad template. This is empty. And then I started building this VFI Customs AI agent solution. This is mocked based on what Craig told me. And this was also pre call. So just working off this screenshot that you sent me but what you see here is that essentially agents at beam work on tasks just like human beings. So they get triggered with an input and they start doing their, I want to call it magic, but it's basically following the SOP or the process flow that we taught them. And then they going to give me an output and either the task is completed, it's input required failed, or that I need some kind of human in a loop step where I need to validate something. Now if we zoom in in one of these tasks, you see 15 days ago, that's the time where Craig and I had the first call. I triggered our agent with one of the request or queries, queries that were I think pretty common for your customs case that you described. And then the agent went ahead and took this input and followed a couple of steps that we taught him to. So in the first step it's going to do the query intake. You see, I mocked custom Maersk tools, I put the Maersk logo on it. As we obviously do not have the Maersk API to your CHP or VFI systems. This is a fully mocked just to give you a rough understanding about how.
Craig Simpson: It would look like now.
Quentin Silvestro: Then the agent goes ahead, takes the query, then retrieves all of the data from the APIs or data warehouses that you want to expose for us and then it does its magic, which is basically the insight generation behind each of these tools. There's a prompt based step or note. So in this case it might look a bit easy, but it's in the end it's a process description. So the exact same flows that I was showing you in the second before, the agent is going to follow these as well. So in the first step it's going to the query intake, it's going to do data retrieval and then in this case the insight generation behind this there is a tool which is essentially a prompt. So you see here best case prompting or best practice prompting behind this, this is autogenerated from our platform already. And here you can start playing around with the variables or tweak the prompt to increase accuracy or output that you want to have. Behind this there are LLMs that we can agnostically choose out of the OpenAI models. We are partnered with IBM and actually presenting one of our invoice reconciliations at IBM in Florida in two weeks. That's why I'm coming, Craig. And this is also why the Watson X model is in here. And then we do a lot of stuff and tropic and plot models as well. Now coming back to my flow. So in the end what we're doing is a workflow builder with AI enhanced Capabilities left and right to set up agents really quickly and to help ensuring that these things run deterministically but also accurate at scale. So a couple of our clients which are in your peer group enterprise I would say they're running tens of thousands of executions of accounts receivables flows or very niche solutions like this one, for example, we need to make sure that the agents run at 95% plus accuracy on these tens of thousands of cases in order to actually make it work as an enterprise grade solution. But maybe I stop here for a second to either answer the first round of questions which you Bennet will probably answer or you can also chip in if you want to additionally Want me to show anything?
Speaker 5: Yeah, I'll go first then. So hope you know the exacto use case we are solving. Right. It's more like a kind of a chatbot, a customer interface when the customer asks some query later to his shipping data. It's more like a test to SQL task. So data is stored in a database or it could be a structured database itself or it could be an ADLS account where unstructured data it's smart enough to pick the information from there and give in a nice representable way. I can say that okay, it's not like a table of format. At some point of time we need to be able to create a kind of a. Kind of a graphs depending upon the kind of data. It could be a pie chart or so it could be a BHAR graph depending upon the kind of the data so that it will be easy a kind of a report for the customer to directly consume the data and I mean for further pre processing. Actually since it is a customer interface data we are more curious on two things actually. The data itself is very sensitive. So we need to make sure the data. How you're going to protect the data without any data leak or anything and how bi information and also how the the platform. Right. Like the interface. So how you protect it from the external threats.
Benedikt Sanftl: Just like.
Speaker 5: Because it's more like a customer going to interact with directly. Right. So prompt injections are in kind of a guardrails. So it would be good that if you have any past experience in that part and so want to know more about it actually. Yeah.
Quentin Silvestro: Cool. Benny, you want to take that one?
Benedikt Sanftl: Yeah, I already raised my hands. I don't know. Does this work right?
Quentin Silvestro: Yes, Yes. I don't see you guys. That's the reason.
Benedikt Sanftl: Yeah. So basically that was a few questions. Let me start from the beginning. The first question was the customer interface so in this case you already built that chatbot or you are looking to build it.
Speaker 5: Yeah, we need to build it, actually.
Benedikt Sanftl: Okay. So basically in this case what you have to do is you have to, you have to use our SDK or API. So for two reasons. First of all, what you saw is the agent builder. So there's a no code agent builder. You don't have to interact with the agent through that platform. There's an API for it where you then can create these tasks or send these messages to the agent. And there you can have like a conversational wrapper around it. So you can have a conversation instruction where you say, hey, you're the Maersk support bot on xyz. You can ask me about shipping information only if you ask me about xyz. What the agent does is it spins off a task, Quentin. If you quickly click on chat, so then maybe you see it. So it spins off a task. But then what you would see here in our interface is basically just the raw execution of the reasoning engine of how a certain task is handled. Like the flow, it could be, you can branch this. So it could be just a one step task, you know, where like there's just one tool call and then the task finishes. It could also be like a branch for a different use case where it then does four things. You know, it's. It could also be like multiple branches, but it can converse and then create a task for a certain specific thing, which could be a single tool or like a chaining of tools. So you have this flexibility of mixing it. What you display to the client in your case then has to be done in a custom interface. Because here in our product, obviously we only show, we show every single step and we will show every single execution, so on. So this is more for debugging and internal use. If you then want to expose this to externals, you would consume in your front end, you would consume the responses, but you maybe don't show the intermediate steps or you rename the intermediate steps where it says, I'm looking up the XYZ database, you maybe don't show to the client. Looking up the XYZ database, you maybe display to your client. You make conversion saying, looking up client data and it trends, it kind of translates, you know, so BIMB is. You use it as an underlying SDK API framework with all the advantages you have from us. Yeah, all of this steer. Which brings me to the second question. The data is basically separated by chat request. So if you send a chat request from someone with a unique you create basically a new chat thread. It is segregated from all the other threads so it doesn't interfere with others. So it's really like a separation. There's just two things. We have a general memory module which can store general information. So within your flow or within your chat you can recall information from that general module. But this is obviously not. There's no information added to the general knowledge if you don't manually add knowledge. So it's also just this is knowledge available to every client who interacts with this agent. The third thing was deployment. So we offered various deployment options. So we have the standard SaaS tool option where you just get a login and you just go to app BIM AI and then it's a multi tenant system. Like you would go to use Atlassian. If you use the cloud version of Atlassian, you just sign up, you know it's on there. We offer like a private instance, but managed by us. Like managed private instance where we spin up a app Maersk Beam AI but it's fully managed by us and there's a VPC to VPC connection. We're fully certified on SOC 2, HIPAA, GDPR, et cetera. But we do the management of the service for you. So we guarantee you uptime, etc. From October onwards. We'll also offer the third thing where you go to your Azure or AWS store and you'll just deploy Beam as a service yourself and then you can use it internally with the same APIs, et cetera. Depending on how much access you then give to our engineers, we can help you in keeping the servers up, debug issues, etc.
Speaker 5: Etc.
Benedikt Sanftl: Usually in the partnership life cycle with clients, we usually don't start with the third option. Right. We'll do somehow a use case or like a proof of value with mock data, like random data first and then later on when we sell out, now we have the concept, this is what we want to build, we do some serious investments, then we roll out like either the second or third option of deployment. Then there was a third question after that which I don't recall now but maybe I answered it already.
Speaker 5: Yeah, yeah, I want to feel. I mean follow up questions. Yes on your response. Actually what I'm understanding is that I mean we gonna build it on top of the the platform. What you're gonna give us Beam AI or this until orchestration everything we need to do it or I mean from your side you're gonna do it. I was bit unclear. Yeah, the first thing. And the second thing is That, I mean, let me complete the.
Benedikt Sanftl: The agent.
Speaker 5: You said it, right. I mean these are. Because see since it's your service provider, right. More likely this agent is going to use the multiple different customers. So how could we ensure that? I mean I was more my question more on the privacy of the data. So how would you ensure that there won't be any data leakage? Because I don't think so. But we need to check whether we can. More like you're expecting the a kind of a solution, a package solution that sits in our servers. Right? That's a kind of solution because if you want to use your third party platform you need to go through a lot of. I mean checks from the most side. So when you take a lot of. What do you say? Process approvals to go forward. Actually that's a kind of just.
Craig Simpson: Just to add some context.
Quentin Silvestro: Right.
Speaker 4: So what we are after, we want to have a conversation on AI as a UI for our customers where they can have conversation with their, with the, with their own data which is relevant for customers clearance.
Benedikt Sanftl: Right.
Speaker 4: So they should be able to ask questions like what is my customs clearance status, where is my shipment currently, etc. And when they ask that they need to have visibility or access to their own data set only.
Benedikt Sanftl: Right.
Craig Simpson: So that's kind of part of what we are after.
Speaker 4: And then how do we ensure that that's the case? So each one, because we're gonna. We have like thousands and thousands of customers if we want to be able to provide this conversation AI to all our customers eventually. And then we want to make sure that they only can have conversation with their specific data.
Benedikt Sanftl: If it makes sense. The answer to this. Let me draw it for you. Okay?
Quentin Silvestro: You want to take over sharing, Vinnie?
Benedikt Sanftl: I already shared, don't I?
Quentin Silvestro: Oh yeah you do.
Benedikt Sanftl: So basically just to recall here again this is the whole stack of Beam. If you see this, that was also in the slides from Quentin. You see here on the bottom you can bring your own LLM endpoints. If at Maersk you already have will make your instance connect to your endpoints. So it's also yours. There is like we have a unified API LLM structure which abstracts all of these away. So you can just plug your LLM here on the bottom. Then we have the Agent framework which is basically the shell of agents tools which is basically. Then tools are used to interact with your databases. Where this information is lying. I'll draw it in a second for you to map it out. Then here on top you see the different hosting versions of this whole Stack in your case, you would go for self hosted or managed service by us because then it's really all the data. The Beam or your Maersk agents then process within one database that you completely own. And you can nuke it if you want so it doesn't leave. It's not mixed with any other company's databases. That's why we offer this managed service or self hosted option. Obviously this comes with an additional cost. So in the case of self hosted, you run, you have the cloud cost, right. For managed service, obviously you have to pay a premium for us managing like a special instance just for you. But yeah, so this is like data wise. Then it's really segregated and you have full control over this like belongs to you. And even on the bottom, which is basically the LLM layer, you can plug your own Azure endpoints or your own. Yeah, whatever Bedrock endpoints providers you already have or use at Maersk, maybe for other things. So there is no mix of data and. Yeah, and then Beam basically itself just. Just another service within your IT environment. Now asking about the segregation of data for your clients, I'll quickly draw this because that's, that's not so easy. So assuming this is our agent for whatever conversation with customs data. So the user would be coming in here. So this is basically the user coming in. He'll send a request, what's my clearance or whatsoever status of my clearance. Sorry if the use case is not perfect. I'm just making this up now on the fly. Okay. So forgive me if the process is not perfect. What the agent would do, it will. Look at this workflow instruction how to do this. So this would be the flow Quentin just showed. So this would be check clearance status. So by having this as like one branch in your flow as a reasoning pattern, this cannot be prompt injected. Right. Because the agent can only pick this flow. And even if you, if they do a prompt injection here saying like let's put this in red prompt injection. The only thing the user could do like hey, what's your system prompt and what can you do? So it would respond with hey, I can do clear check clearance status. I can do xy that I can do xyz. But they cannot access the inside of this because it's protected through this layer here. Okay, so let's go back to the node path. They asked the user ask for my clearance. It checks the clearance status and then the workflow executes. So this guy executes and this guy then has access to your, let's say shipping data and then probably also has access to the customs customs data, maybe even has access to, maybe the flow is even to check is this even a customer first, your internal customer, legit customer? So maybe this is even the first thing we would do. And yeah, and then based on collecting all of this information in your flow, the result will go back to the agent and then the agent will reformulate it and answer you with the answer response to client, your shipment is still in customs and awaits payment to get cleared. So this is what it would look like and what Beam provides basically is everything here on the right. So all of this here is Beam. And now how you display this here to the user, this is the tricky question which I stated in the beginning, because what happens here is basically in between is you could show all these steps here, right? So the question is, do you want to show to the user that you're checking with your chatbot that hey, are you a legit customer? So I don't think you want to show this in the conversation, but maybe you want to show to the client that hey, I'm checking the shipment data. Hey, I'm checking the custom data. Hey, and now here's the full response. And this is kind of exactly the level of detail, but it all comes here on the API level. And this is where we then where you then integrate with our API of Beam and then you define what stream you consume and how you display it to the user in the front end. Last question is how do we know the user? Who is the user? So this has to be basically come with the request. So as like as part of the request here you need to send some kind of UID of the user. Alternatively you could also do something like hey, give me your email address and your shipment number. If you think that's strong enough as like, I don't know, you know, with DHL and others you can put your shipping trackman number and just your name and already gives you a lot of information. If you say this is enough, we can also just instruct the agent to ask for the shipment number and the three things. And then this information would be used to call these APIs, because these APIs here basically then on the API call we pass on exactly this UID here and here. And this is how we verify that only the user who either registered or provided the information can access the information in the database. And obviously you as MERS control this data layer here. By defining these API and which APIs to expose to the Beam stack, you define what the agent has access to. So we call it custom integration. So you basically write what kind of. Okay, there's a check clearance status and the agent has to give the UID and the time frame to look for and XYZ and if. Yeah, depending on what role access level you give on that API only the agent can only access that information. Only if it's a valid uid, it actually gives that back. Yeah. And then we can put different layers on top. Obviously we can do this with session cookies. So if you have a login screen here in the front, you could retrieve a session cookie, etc. Like a typical access token and you pass on the access token as a UUID and then this access token here on the other side is checked against your authentication service. Is this an active access token which has been retrieved? You know, you plug it in here as like a, as a superficial level on the access token. Does, does like, does this make sense? Does this answer your question?
Speaker 5: Yeah, I think the Nikola question exactly the same thing. Because you have multiple customers and we need to make sure that the right information go to the right customer. Right. It should not. Yes. If you ask a customer, you should not get a customer. B because this is a chartboard, so there is a high chance of prompt injections. We can fake the LLMs. You know better it actually.
Benedikt Sanftl: Right.
Speaker 5: So it should not, it should not return the, the customer base information actually. So from the beam from your side, do you have an additional cord Rails or do you have any experience in this kind of use cases? I think that's, that's the curious question to ask actually.
Benedikt Sanftl: Yeah, I mean the first thing of all it's like through this abstraction layer here, as here is basically like an instruction in here. It's like you are a customer support response agent. You have a flow available to call when use case X Y Z. So this is kind of baked here into the instruction, right. And when it hits this intent, it then does basically this, this game here on the right side. So the agent instruction itself is not aware itself of the APIs or any of the underlying stuff. It only knows that what his role is and what basically it's supposed to do. If someone does a prompt injection here and extracts the system prompt, he can't do anything with it because the only thing he learns is the, the role and what the agent can do. But the underlying thing he can then tell it obviously to say like, hey, place a shipment for $1 million. Obviously you can say this, but if the flow doesn't support this and the APIs are not there, nothing will happen. He can try if we add this top level, what I said here to access the API there is like an API key, right? So if you want to access the API key you need an active access token. The API you need an active access token. So you somehow need to be a user. So you need to log in first and then you can put API requests on top of it, maximum rate limits, etc. If you don't put like a login button, but you make it part of the conversation, the user always has to go through the conversation first, providing is in information and then you can. So this depends a bit on the use case. So but like this year, the system prompt can be leaked. There's no information in the system prompt, right? It's just the right side which needs to be protected. So it's kind of like chatgpt. Nobody really cares about the system prompt as long as the. The connectors on the right side, the authentication layer on that level that is protected and that beam does. So we have a complete so for these APIs here there is a complete oauth whatsoever API level connector thing you have implemented. So we can show it in the. In the product. So here if you see if you connect an integration, you basically have to add a connection with. Integrate with your credentials. So to access an API also here you have to provide your. Even if it's basically like an internal, you have to provide all of these security mechanisms and once you revoke it on your side, then obviously you can also monitor the access level on your side as you can monitor your own API calls, etc. But this is how you segregate it from a user data security standpoint. It took us quite some while to get there, to be honest.
Craig Simpson: Got some questions pertaining to something Quinton already mentioned about enterprise customers really rely on high accuracy and ultimately what we're trying to do is provide a good customer experience. So similar to accuracy, can you comment or elaborate a little bit more on the latency as well? Because I think getting a response, an accurate response 1, but also a timely response is very important too. So a question around that is. Well, one, what's your outlook in terms of latency? But two, how do you also track that in terms of the metrics for both accuracy I've seen generally on each of the steps. But do you have more of an overarching view in terms of like a dashboard in what tracks the metrics for accuracy, latency and also the cost part as well? I think Anil, you mentioned is quite important previously too.
Benedikt Sanftl: So we do track this. Here you can see kind of, yeah, if you implement the feedback API in the front end, you can even see the scores. Right. So I mean, you can also rate them within the system, but if you put the feedback, then you could also see the overview. Yeah, then we see the average scores of the internal evaluation, like the scorer. The internal scorer, how many tasks failed. You can also see how long the agent worked in total, how long one runtime is. Obviously this is now not representative of the final use case because the direct chat response comes immediately, but completing the whole task might take a minute or two. There's even tasks where we have a user consent. So you could even say, like on certain of your branches or flows, someone of Maersk has to come in and approve this first before the response gets sent back to the client. So in these cases, maybe, and I think in this case we had a few tasks when Quentin exercised that he had to click an approval first. So this then bumps up the time. But like for critical paths, like let's say shipments above 10 million or something, you don't want to just give out the answer. You want to have someone from your team sit and get a notification in Slack or team saying like, hey, someone requests to look at this data and you can like, okay, yeah, press yes. And then it only. The agent only continues. So this is. This obviously delays the time, but you can do it. And on the direct chat answer, so you, you send the message, you immediately get a response that we are now working on the task. So you, you know, when in chatgpt, you re. You use the Deep Research module. So you ask something and this card pops up so you can style it in the same way, saying like, hey, we are chatting now. So you directly get this back and forth, back and forth, back and forth. And then once a task gets hit, like a longer workflow, which is four or five steps, you can just show a card saying like, hey, I'm executing your request. It's four or five steps. And this is maybe all the information you show your client or you show the metadata of. This is the. The name of the step, right? So you show them like the first step, second step, third step. So he gets a continuous update, like what the agent is doing. And this is how you bridge basically the interaction with the client. Even maybe the whole task takes two minutes because, yeah, it's 17 steps or something, you know, so we have some clients where we do order requests and it's 17 steps because it's so nested. Then obviously the task takes Five minutes. But if you, you then have to update the client like, hey, but this is kind of what you have to do in the custom front end of your chatbot.
Craig Simpson: Understood. Okay.
Benedikt Sanftl: Unfortunately there's something we can't give you out of the box because every client wants this different. Right. But it's not that hard to build. It's easy.
Craig Simpson: And then I can't remember if we did speak about this last time, Quintin, but it was alluded to earlier on in the call in terms of generating reports and perhaps if we do have integration or can integrate to downstream outputs, for example Power bi, that's something we use the data lakes Azure based. So how does that look like from your perspective too?
Benedikt Sanftl: So there's an analytics endpoint which you can just query, which gives you high level analytics by date from the agent. So how many tasks, feedback, etc. Basically what I showed in the UI as well. You can just connect Power BI as well there. We can expose anything you want if you tell us, look, this is the data we want to measure in our Power bi because we don't want to look into the Beam tool to get the data. Just tell us what endpoints you exactly need and we just add them here. Or we add a specific agent tasks Power BI endpoint, which is a bit different to this. But you can see here in the callback, almost everything is in there, which we showed before as well. Yeah, and you define it by period. Right. So you can see here. Yeah, we have ourselves Mixpanel integrated. So I don't know if you use Mixpanel. Yeah, maybe we ship it to you anyway. So you get mixed panels out of the box as well if you want.
Craig Simpson: We get reports directly within the chat. So we're talking about integration with Power bi, which could be a use case that we might use perhaps more internally speaking, but for the customers aspect. Like is the possibility of generating reports or even. Yeah, let's just say reports, PDFs, maybe even a slide deck or something. Just thinking outside the box of what potentially they might want. But yeah, it just thinking more so without necessarily taking the additional step to integrate with a Power bi. Can we have the like. Okay, I requested, can you generate a report of my customs clearances for the last two months and populate this as a bar chart or something like that and then a downloadable PDF is the endpoint.
Benedikt Sanftl: Yes, this is correct. It's very easy to do this. The thing is you would need to write the renderer in the front end. So basically we could say there is a Flow in your agent saying like, this is what you need to do for a report, and then it creates the PDF. So the last step would be like a Create PDF and this one would create the PDF and then would send back the PDF metadata to the front end, and then the front end has to show it as a PDF and then you can just download it. So it sends the blob, or it creates it on AWS blob storage and just sends the link back. And then the user can click on the link and get its PDF. This is kind of possible, but obviously the data needs to be available for the agent to query before. So it would be a case report generation would be one branch and then it would be like, okay, get the information from that database based on the query and then generate the PDF. And then, yeah, you get the response back.
Craig Simpson: Understood. All right.
Benedikt Sanftl: There is some custom work to do. You can see like, you know, but of course, it's all. It's possible. And this is, by the way, the most. This is like, if you look at the slide of what Quentin showed or.
Quentin Silvestro: Built, they haven't seen the text slides yet. Because I meant to follow up after this call with the text slides.
Benedikt Sanftl: Okay, so setting up the initial version of the agent and like the base use case, kind of like, hey, chat, and then giving it all of this skills, that is basically the time, which is we call optimization of the agent. So basically you build the rough thing very fast and then you iterate on it. And then once you have the iterated version, you then test it with users to collect edge cases to then optimize your agent. And this is kind of the journey you go through. I can quickly show you. This is basically. This is how it looks like. Cc. I'm not. I'm doing this a lot. Not the first time. I did, like drawings. Yeah. So this is kind of the journey you go through. See, we discovered the agent together. We build the initial version and then we. We test it and eventually we deploy it. And then we collect feedback from the actual internal users, then external users, and then use this to continuously improve our agent. So it could be a test batch of users. Like, then like this, you scale up first like a lead customer or internal team, then lead customer 10, and then everybody somewhere. And then. Yeah, and then it becomes kind of this continuous improvement flywheel. Okay.
Craig Simpson: All right, thank you very much. Just to give you an update in terms of where we are internally speaking is this past week or so, I would say we are still evaluating different partners in this. And then ultimately still getting to the decision where, and I think I alluded to this in our previous call, Quinton, in that we generally have the capabilities to do this internal. Anil's actually been working on some similar projects for other departments within Maersk and successfully successfully deployed a chatbot similar to what we kind of envisaged this use case to be. So I think once we've completed this overall review, one of the questions that are to be answered is do we do this internally or externally? I can't say for certain which outcome will be. And then within that, of course we'll need to determine or at least aggregate all this information that we've collected from the likes of yourselves as well to make a more informed decision of which direction we would want to go in in terms of choice. And then I do expect some follow up calls as well. Particularly I think what would be important if we were to proceed with Beam is for you to have an overview in terms of what this VFI track platform looks like, how it stands today, how customers are using it today, more so as well. And then I think you'll get a better understanding of what the information shows or what information exists first and foremost, but also what the custom customer can get out of it as of today and how we're trying to take this to the next level. So I know we've explained a little bit about the use case, but more importantly, as a subsequent step before we even move this to say proof of value would be important just for your eyes on how this looks like today.
Benedikt Sanftl: Yeah.
Quentin Silvestro: If I may ask a what could you elaborate a bit on your decision criteria for this buy or build decision? Plus, if you may want to answer for me, would be cool to understand what other vendors you're looking on. So also Ben, from a technical standpoint can differentiate what our take is and how we're different.
Craig Simpson: Yeah, I know you sent over that list or your feedback from the last review. I think there's a couple of.
Quentin Silvestro: Yeah, you mentioned them already. Sorry, I forgot.
Craig Simpson: Yeah, there was, I think there was a couple more that might have not existed in the last time we spoke, but I can't remember off the top of my head. I think the only. The new one for sure is Core with a K. Yeah. Or Core A. I don't know how they're officially pronounced, but yeah, in terms of the build versus buy, that's, that's really a conversation about criteria. That's tbc. That's the purpose of our follow up question, to say basis on what we have learned, does this give us more confidence to do it one way or another. And so what exactly that is in the back end decision making. I really need to leverage the likes of Anil to understand like what. What needs to be true for us to do this externally.
Quentin Silvestro: Yeah, understood. Bennett, just now that we are here in the call and we have the time, do you want to share your take on what makes Beam special and how we're different? So Craig also hears it from the person whose crime this is.
Benedikt Sanftl: I mean when you built it internally you probably used all of the open source frameworks and all the stuff out there and plugged it together. That's like my assumption. But you don't have to comment. But you run in some issues on retry mechanisms, hallucinations. Yeah, kind of resolved all of this in our framework. Like we abstracted this. So once you hit our API endpoints, the retries and all of that stuff, rate limit, back offs, etc. Etc. It's already implemented kind of this is what we built, this is why we built it ourselves. Because LangChain and all of that stuff was not productized or is not ready enough to actually be used in production environment. So looking for like an enterprise framework which already solves some of these things is, is my opinion a good starting point for not building it completely yourself because then you have to solve all of the problems we had already. Again obviously it comes also with downsides. Yeah, if you build it yourself you have full control over it. Then for BIM AI, what, what makes it special? So we control the whole stack. I showed before like from bottom to top it's like comes out of one box and then if you have special requests we just put it into the framework as like an API. And as we're now going more API first you will have like a developer friendly way to build agents. But at the same time business people can use our already existing no code interface to also check the agents and make small edits themselves so they don't need to go through to code, they can just go there and see like ah, I want to just tweak this a bit. And then business people or like automation people can later maintain it without engineering teams going back through the whole deployment circle of code. I think that's a thing. And then all of these features with feedback and then we're going to ship something we call like agent tuning, self learning. We want the agent later on to learn from client feedback. But I would say this is more like an outlook. We started with our first beta clients. It's A little bit. We still try to figure out what's the best use of it. So some clients just say like hey, just put it there. They assume I just learns by itself and then it just gets better over time. But the reality is we actually want to collect feedback and then some person look at, choose the right feedback, click a button, then I improve itself, then you approve the self improvement and and then you have kind of a human in the loop in this. So we are experimenting with this, how we can steepen this later part of the building, you know, which I mentioned before. And we gotta ship this as features because I think that's where the most time is spent in development. It's this first prototype you see goes out very fast.
Craig Simpson: But yeah, we'll need multiple iterations I would assume. Yes.
Benedikt Sanftl: Okay.
Craig Simpson: No, no, thanks.
Benedikt Sanftl: And obviously there we have this for agent deployed engineers. So we have this team which supports, you know, so we have worked with like multiple, I don't know, 30 plus clients. So we know what works, what doesn't. And yeah, I mean we even some we for clients, we even do the whole building. Right. So I'm not sure if we should do the front end building for your team, but usually the whole agent building we offer as a service as well on top of so you don't have to have an expert, but we can also we are open to like just training internal agent builders or actually giving you the service of building the complete agent. You just don't have to integrate it with your environment. For some people this is slack. For you it will be a chatbot who's hosted on I think like a widget in your homepage or internal client section whatsoever.
Craig Simpson: Okay, good to know. I'm just taking down some notes.
Quentin Silvestro: I will also follow up with all of this and give you a condensed debrief. Cool, sounds good. As said, you will get an email from me until end of week latest where I compile of the information that we discussed today and also sent you the text slides to debrief. If there's anything you want us to follow up on and go deeper, just feel free to send us an email and we can arrange the call. Anyway, besides that last question, what's a rough timeline that you're looking at?
Craig Simpson: Yeah, we're hoping to connect as a team internally early next week just to have our initial review. In an ideal scenario we would get to a conclusion of okay external and then this partner. Realistically speaking, I don't think we're at that stage yet because we've realized that say this 30 minute engagements isn't exactly enough time to really make a fully informed opinion based on what makes more sense from a solution perspective. So I would anticipate that there's going to be perhaps another follow up call with like, okay, we've now aligned with instead of these x many we've got down to maybe two and then we'll need to have a follow up call before we even get to the final selection. And then from there it would be the case where we could start designing the pilot poc, the critical success criteria we're talking about like the latency, the accuracy, etc. Etc. And then kicking that POC off. And I understand, as you rightly pointed out, the first version could be done very quickly and then we would iterate and look to say something testing this out for, I don't know, off the top of my head, maybe say six weeks to at least allow customers to be able to use it and us to also get feedback from the customers in terms of their experience. But immediate, immediate steps are the internal review discussion next week and then depending on the conclusions or outcomes from that call, then we'll need to likely schedule a follow up call which will include of course the demo of the platform and then maybe some deeper dive questions into okay, how does this make sense or how do we make this work? Put it that way. Cool.
Quentin Silvestro: Okay then allow me some time to compile all of the information that we discussed today and make Ben as quick drawings. Nice.
Benedikt Sanftl: You have access to my quick drawings?
Quentin Silvestro: No, you need to share them so you can make nice figma slides out of it.
Benedikt Sanftl: See, this always works. I draw something and then it becomes nice and shiny later. Cool.
Quentin Silvestro: Thank you Craig. We'll follow up.
Craig Simpson: Yeah, thank you both. Have a good rest of your day. Cool, Cheers. Bye Bye.